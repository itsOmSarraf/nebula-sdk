# Stream Chat

Stream real-time responses from the AI model for better user experience.

## Method

```typescript
chat.stream(request: ChatRequest): AsyncIterable<ChatChunk>
```

### Parameters

<ParamField path="request" type="ChatRequest" required>
  The chat request configuration
</ParamField>

<ParamField path="request.message" type="string" required>
  The user message to send to the AI
</ParamField>

<ParamField path="request.systemPrompt" type="string">
  System prompt to guide the AI's behavior
</ParamField>

<ParamField path="request.context" type="ChatMessage[]">
  Previous conversation history for context
</ParamField>

<ParamField path="request.temperature" type="number">
  Override the default temperature for this request
</ParamField>

<ParamField path="request.maxTokens" type="number">
  Override the default max tokens for this request
</ParamField>

## Response

Returns an `AsyncIterable<ChatChunk>` that yields streaming response chunks.

### ChatChunk Properties

<ResponseField name="content" type="string">
  The text content of this chunk
</ResponseField>

<ResponseField name="delta" type="string">
  The incremental text added in this chunk
</ResponseField>

<ResponseField name="finished" type="boolean">
  Whether this is the final chunk
</ResponseField>

<ResponseField name="usage" type="TokenUsage">
  Token usage information (only in final chunk)
</ResponseField>

## Example

<CodeGroup>

```typescript Basic Streaming
import { createAgent } from '@src/index';

const agent = await createAgent({
  name: 'Storyteller',
  providerAddress: '0xf07240Efa67755B5311bc75784a061eDB47165Dd', // llama-3.3-70b-instruct
  memoryBucket: 'storyteller-memory',
  privateKey: 'your-private-key'
});

async function streamChat() {
  agent.setSystemPrompt('You are a creative storyteller.');
  
  const response = await agent.streamChat(
    'Tell me a story about AI',
    (chunk) => {
      process.stdout.write(chunk);
    }
  );
  
  console.log('\n\nStream completed!');
  console.log('Complete story:', response);
}

streamChat();
```

```typescript With Context
import { createAgent } from '@src/index';

const agent = await createAgent({
  name: 'Programming Tutor',
  providerAddress: '0x3feE5a4dd5FDb8a32dDA97Bed899830605dBD9D3', // deepseek-r1-70b for reasoning
  memoryBucket: 'tutor-memory',
  privateKey: 'your-private-key'
});

async function tutorWithContext() {
  agent.setSystemPrompt('You are a helpful programming tutor.');
  
  // Previous conversation is automatically maintained in memory
  await agent.ask('Hello, I need help with coding');
  
  // This will include the previous context
  const response = await agent.streamChat(
    'I want to learn TypeScript',
    (chunk) => {
      process.stdout.write(chunk);
    }
  );
  
  console.log('\nComplete response:', response);
}

tutorWithContext();
```

```typescript React Integration
import { useState, useEffect } from 'react';
import { createAgent } from '@src/index';

function ChatComponent() {
  const [response, setResponse] = useState('');
  const [isStreaming, setIsStreaming] = useState(false);
  const [agent, setAgent] = useState(null);
  
  useEffect(() => {
    async function initAgent() {
      const newAgent = await createAgent({
        name: 'Chat Assistant',
        providerAddress: '0xf07240Efa67755B5311bc75784a061eDB47165Dd',
        memoryBucket: 'chat-memory',
        privateKey: 'your-private-key'
      });
      newAgent.setSystemPrompt('Be helpful and concise.');
      setAgent(newAgent);
    }
    
    initAgent();
  }, []);
  
  const handleStream = async (message: string) => {
    if (!agent) return;
    
    setIsStreaming(true);
    setResponse('');
    
    try {
      await agent.streamChat(message, (chunk) => {
        setResponse(prev => prev + chunk);
      });
    } catch (error) {
      console.error('Streaming error:', error);
    } finally {
      setIsStreaming(false);
    }
  };
  
  return (
    <div>
      <div>{response}</div>
      {isStreaming && <div>AI is typing...</div>}
    </div>
  );
}
```

</CodeGroup>

## Advanced Usage

### Custom Temperature per Request

```typescript
const stream = await chat.stream({
  message: 'Write a creative poem',
  temperature: 0.9, // High creativity
  systemPrompt: 'You are a poet.'
});
```

### Handling Errors

```typescript
try {
  const stream = await chat.stream({
    message: 'Hello',
    systemPrompt: 'Be helpful'
  });
  
  for await (const chunk of stream) {
    console.log(chunk.delta);
  }
} catch (error) {
  if (error.code === 'RATE_LIMIT') {
    console.log('Rate limit exceeded, please wait');
  } else if (error.code === 'NETWORK_ERROR') {
    console.log('Network error, please check connection');
  } else {
    console.log('Unexpected error:', error.message);
  }
}
```

### Cancelling Streams

```typescript
const controller = new AbortController();

// Cancel after 10 seconds
setTimeout(() => controller.abort(), 10000);

try {
  const stream = await chat.stream({
    message: 'Tell me a very long story',
    signal: controller.signal
  });
  
  for await (const chunk of stream) {
    console.log(chunk.delta);
  }
} catch (error) {
  if (error.name === 'AbortError') {
    console.log('Stream was cancelled');
  }
}
```

## Performance Tips

1. **Buffer chunks** for better UI performance in web applications
2. **Use debouncing** when displaying streaming text
3. **Handle network interruptions** gracefully
4. **Implement retry logic** for failed streams

## Error Codes

| Code | Description | Action |
|------|-------------|---------|
| `RATE_LIMIT` | Too many requests | Wait and retry |
| `INVALID_API_KEY` | Authentication failed | Check API key |
| `NETWORK_ERROR` | Connection issues | Retry request |
| `CONTENT_FILTER` | Content blocked | Modify message |
| `TOKEN_LIMIT` | Response too long | Reduce max tokens |

## Related

- [Send Single Messages](/api-reference/chat/create)
- [View Chat History](/api-reference/chat/history)
- [Memory Integration](/api-reference/memory/store)
